{"name":"Practicalmachinelearning","tagline":"Practical Machine Learning - Repository for Project","body":"Practical Machine Learning - Project\r\n========================================================\r\nThe data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. \r\n\r\nWhat you should submit\r\n\r\nThe goal of your project is to predict the manner in which they did the exercise. This is the \"classe\" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. \r\n\r\n1. Your submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).\r\n2. You should also apply your machine learning algorithm to the 20 test cases available in the test data above. Please submit your predictions in appropriate format to the programming assignment for automated grading. See the programming assignment for additional details. \r\n\r\n### Download file \r\n\r\n\r\n```r\r\nif(!file.exists(\"./data\")){dir.create(\"./data\")}\r\nfileUrl <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\ndownload.file(fileUrl, destfile=\"./data/pml-training.csv\")\r\nfileUrl <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\ndownload.file(fileUrl, destfile=\"./data/pml-testing.csv\")\r\n```\r\n### Load data into R.\r\nThe directory data in the working spoace is created, if it not exists:\r\n\r\n\r\n```r\r\nset.seed(975)\r\npmltraining <- read.table(\"./data/pml-training.csv\", sep = \",\", header=T, stringsAsFactors=F)\r\npmltesting <- read.table(\"./data/pml-testing.csv\", sep = \",\", header=T, stringsAsFactors=F)\r\n```\r\n\r\n### Cleanig the dataset:\r\n\r\nIn order to identify the columns really needed, was used used the documentats from the [Human Activity Recognition Project](http://groupware.les.inf.puc-rio.br/har)\r\n\r\n#### All credits to: \r\n#### Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. [Document](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf)\r\n\r\nThe strategy chosen is to remove any variable which isn't a value captured  by sensors, and so the first 7 columns are deleted:\r\n\r\n1. Index \r\n2. user_name\t\r\n3. raw_timestamp_part_1\t\r\n4. raw_timestamp_part_2\t\r\n5. cvtd_timestamp\t\r\n6. new_window\t\r\n7. num_window\r\n\r\n\r\n```r\r\npmltraining <- pmltraining[,8:ncol(pmltraining)] \r\npmltesting <- pmltesting[,8:ncol(pmltesting)] \r\n```\r\n\r\n### The second step to cleaning the dataset id to remove the column with NA values:\r\n\r\n\r\n```r\r\npmltraining <- pmltraining[,colSums(is.na(pmltraining)) == 0]\r\npmltesting <- pmltesting[,colSums(is.na(pmltesting)) == 0]\r\n```\r\n\r\n### The third step is to remove the columns with character type values, not needed for the model fitting.\r\n\r\n#### Checking char columns and removing except \"classe\" from dataset pmltraining. The final dataset is made by 52 numeric columns and the classe character column.\r\n\r\n```r\r\nch <- sapply(names(pmltraining), function(x) inherits(pmltraining[,x], c(\"character\")))\r\nch <- names(which(ch==TRUE))\r\nch <- ch[!ch == \"classe\"] \r\npmltraining <- pmltraining[,-which(names(pmltraining) %in% ch)]\r\n```\r\n\r\nMaking classe as factor in order to use it as a outcome variable in the model:\r\n\r\n```r\r\npmltraining$classe <- as.factor(pmltraining$classe)\r\n```\r\n\r\n####  Exploratory Data Analysis\r\n\r\nThese are the final variables and the distribution of classe response for three variable explaining total data:\r\n\r\n1. total_accel_belt\r\n2. total_accel_arm\r\n3. total_accel_dumbbell\r\n4. total_accel_forearm\r\n\r\n```r\r\npar(mfrow=c(2,2))\r\nplot(pmltraining$classe ~ pmltraining$total_accel_forearm)\r\nplot(pmltraining$classe ~ pmltraining$total_accel_arm)\r\nplot(pmltraining$classe ~ pmltraining$total_accel_dumbbell)\r\nplot(pmltraining$classe ~ pmltraining$total_accel_forearm)\r\n```\r\n\r\n![plot of chunk unnamed-chunk-7](figure/unnamed-chunk-7.png) \r\n\r\n{ figure/unnamed-chunk-7.png }\r\n\r\n### Building training model.\r\n\r\n#### We choose to train the model using Random Forest Algorithm to target a high level of  accuracy. The train control method chosen is oob (Out of Bag). this method perform a high prediction power, and really good performance in computing the model.\r\n\r\nAccording to the [oob document](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr) , this method performs an unbiased estimate internally, so there is no need for cross-validation or a separate test set to get an unbiased estimate.\r\n\r\nAnyway, we choose to create the training and test set, in order to renforce the reliable of the result, and demonstrate the application of consepts.\r\n\r\n#### Making a train set and a  test set from the pmltraining data set  to validate expected out of sample error, and accuracy.\r\n\r\n\r\n```r\r\nlibrary(caret)\r\n```\r\n\r\n```\r\n## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n```\r\n\r\n```r\r\ndp <- createDataPartition(y = pmltraining$classe, p=0.2,list=FALSE) \r\ndpdatatest <- pmltraining[dp,]\r\ndpdatatrain <- pmltraining[-dp,]\r\nnrow(dpdatatest); nrow(dpdatatrain)\r\n```\r\n\r\n```\r\n## [1] 3927\r\n```\r\n\r\n```\r\n## [1] 15695\r\n```\r\n\r\n#### Training the predictor model:\r\n\r\n\r\n```r\r\nmodFitrf_train <- train(classe ~ ., data = dpdatatrain, method=\"rf\", trControl = trainControl(method = \"oob\", number = 4)) \r\n```\r\n\r\n```\r\n## Loading required package: randomForest\r\n## randomForest 4.6-7\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n```\r\n#### Plotting the final model:\r\n\r\n```r\r\nplot(modFitrf_train$finalModel)\r\n```\r\n\r\n![plot of chunk unnamed-chunk-10](figure/unnamed-chunk-10.png) \r\n\r\n\r\n#### Checking and validating the model using confusionMatrix and postResample method. \r\n\r\n\r\n```r\r\ncm <- confusionMatrix(dpdatatest$classe, predict(modFitrf_train, dpdatatest))\r\ncm\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1115    1    0    0    0\r\n##          B    4  756    0    0    0\r\n##          C    0    3  676    6    0\r\n##          D    0    0    0  644    0\r\n##          E    0    0    0    0  722\r\n## \r\n## Overall Statistics\r\n##                                         \r\n##                Accuracy : 0.996         \r\n##                  95% CI : (0.994, 0.998)\r\n##     No Information Rate : 0.285         \r\n##     P-Value [Acc > NIR] : <2e-16        \r\n##                                         \r\n##                   Kappa : 0.995         \r\n##  Mcnemar's Test P-Value : NA            \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             0.996    0.995    1.000    0.991    1.000\r\n## Specificity             1.000    0.999    0.997    1.000    1.000\r\n## Pos Pred Value          0.999    0.995    0.987    1.000    1.000\r\n## Neg Pred Value          0.999    0.999    1.000    0.998    1.000\r\n## Prevalence              0.285    0.194    0.172    0.166    0.184\r\n## Detection Rate          0.284    0.193    0.172    0.164    0.184\r\n## Detection Prevalence    0.284    0.194    0.174    0.164    0.184\r\n## Balanced Accuracy       0.998    0.997    0.999    0.995    1.000\r\n```\r\n\r\n```r\r\npr <- postResample(predict(modFitrf_train, dpdatatest), dpdatatest$classe)\r\npr\r\n```\r\n\r\n```\r\n## Accuracy    Kappa \r\n##   0.9964   0.9955\r\n```\r\n\r\n### We observe an accuracy of 0.996, and an estimate of OOB error rate of 0.6% as follow:\r\n\r\n\r\n```r\r\nmodFitrf_train$finalModel\r\n```\r\n\r\n```\r\n## \r\n## Call:\r\n##  randomForest(x = x, y = y, mtry = param$mtry) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 27\r\n## \r\n##         OOB estimate of  error rate: 0.6%\r\n## Confusion matrix:\r\n##      A    B    C    D    E class.error\r\n## A 4459    3    1    0    1    0.001120\r\n## B   19 3010    8    0    0    0.008890\r\n## C    0   16 2717    4    0    0.007307\r\n## D    0    1   27 2541    3    0.012053\r\n## E    0    1    3    7 2874    0.003813\r\n```\r\n\r\n\r\n###  Predicting values for pml-testing \r\n\r\n#### Make prediction\r\n\r\n```r\r\npred <- predict(modFitrf_train, pmltesting)\r\n```\r\n\r\n#### Submitting function ( 20/20 correct).\r\n\r\n\r\n\r\n```r\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\npml_write_files(pred)\r\n```\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}